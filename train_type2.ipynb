{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROUTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise = 'exer1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('dataset/exer1/seq/seq_little_1670082995.npy'),\n",
       " WindowsPath('dataset/exer1/seq/seq_little_1670083079.npy'),\n",
       " WindowsPath('dataset/exer1/seq/seq_thumb_1670082995.npy'),\n",
       " WindowsPath('dataset/exer1/seq/seq_thumb_1670083079.npy')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_set_group = sorted([x for x in Path(f\"./dataset/{exercise}/seq/\").glob(\"*.npy\")])\n",
    "tr_set_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['thumb','paper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for fle in tr_set_group:\n",
    "    if len(data) == 0:\n",
    "        data = np.load(fle)\n",
    "    else:\n",
    "        data = np.concatenate([data, np.load(fle)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2055, 30, 100)\n",
      "[ 6.25918388e-01  5.90298712e-01 -3.07661480e-07  0.00000000e+00\n",
      "  6.49076819e-01  5.44179380e-01  5.23972558e-03  0.00000000e+00\n",
      "  6.53718233e-01  4.96857941e-01  1.15060275e-02  0.00000000e+00\n",
      "  6.37362599e-01  4.65235889e-01  1.66208260e-02  0.00000000e+00\n",
      "  6.18655205e-01  4.54011291e-01  2.18164902e-02  0.00000000e+00\n",
      "  6.07456863e-01  4.38670427e-01  1.07005425e-02  0.00000000e+00\n",
      "  5.80796480e-01  4.27315176e-01  1.37827946e-02  0.00000000e+00\n",
      "  5.73692560e-01  4.40656185e-01  1.61442272e-02  0.00000000e+00\n",
      "  5.76137722e-01  4.56456125e-01  1.76963247e-02  0.00000000e+00\n",
      "  5.83384514e-01  4.48403656e-01  7.10306922e-03  0.00000000e+00\n",
      "  5.61155796e-01  4.51364219e-01  8.26031715e-03  0.00000000e+00\n",
      "  5.62662721e-01  4.72303987e-01  7.44483108e-03  0.00000000e+00\n",
      "  5.69637895e-01  4.88713950e-01  6.33986155e-03  0.00000000e+00\n",
      "  5.64535856e-01  4.71386284e-01  3.58748296e-03  0.00000000e+00\n",
      "  5.38593471e-01  4.69211459e-01 -2.49453378e-03  0.00000000e+00\n",
      "  5.37776411e-01  4.86159980e-01 -6.35221554e-03  0.00000000e+00\n",
      "  5.43579161e-01  4.97828305e-01 -8.28573946e-03  0.00000000e+00\n",
      "  5.49983919e-01  5.01738310e-01  1.28210662e-03  0.00000000e+00\n",
      "  5.15808105e-01  4.76670802e-01 -9.71484836e-03  0.00000000e+00\n",
      "  4.90348607e-01  4.64175463e-01 -1.57470647e-02  0.00000000e+00\n",
      "  4.70247269e-01  4.51309472e-01 -1.87161695e-02  0.00000000e+00\n",
      "  2.09868641e+01  3.26406860e+01  3.15357800e+01  5.97646484e+01\n",
      "  8.41831665e+01  3.66766434e+01  8.07794189e+01  8.66518555e+01\n",
      "  1.89336548e+01  5.92624016e+01  8.90394287e+01  2.90132160e+01\n",
      "  1.99773235e+01  1.01677990e+01  8.06079102e+00  1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2055, 30, 99)\n",
      "(2055,)\n"
     ]
    }
   ],
   "source": [
    "x_data = data[:,:,:-1]\n",
    "labels = data[:,0,-1]\n",
    "\n",
    "print(x_data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2055, 2)\n"
     ]
    }
   ],
   "source": [
    "y_data = to_categorical(labels, num_classes=len(actions))\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1849, 30, 99) (1849, 2)\n",
      "(206, 30, 99) (206, 2)\n"
     ]
    }
   ],
   "source": [
    "x_data = x_data.astype(np.float32)\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "x_tr,x_val,y_tr,y_val = train_test_split(x_data, y_data, test_size=0.1, shuffle=True, random_state= 3)\n",
    "\n",
    "print(x_tr.shape, y_tr.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 99)\n"
     ]
    }
   ],
   "source": [
    "print(x_tr.shape[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 64)                41984     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,626\n",
      "Trainable params: 44,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    LSTM(64, activation = 'relu', input_shape = x_tr.shape[1:3]),\n",
    "    Dense(32, activation = 'relu'),\n",
    "    Dense(16, activation ='relu'),\n",
    "    Dense(len(actions), activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['acc']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 32.8993 - acc: 0.5744\n",
      "Epoch 1: val_acc improved from -inf to 0.55825, saving model to models/exer1\\classifier_acc.h5\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 29.72185, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 3s 23ms/step - loss: 33.2096 - acc: 0.5717 - val_loss: 29.7218 - val_acc: 0.5583 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 13.9300 - acc: 0.6412\n",
      "Epoch 2: val_acc improved from 0.55825 to 0.75728, saving model to models/exer1\\classifier_acc.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 29.72185 to 8.58111, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 13.7252 - acc: 0.6425 - val_loss: 8.5811 - val_acc: 0.7573 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 16.0771 - acc: 0.6245\n",
      "Epoch 3: val_acc did not improve from 0.75728\n",
      "\n",
      "Epoch 3: val_loss did not improve from 8.58111\n",
      "58/58 [==============================] - 1s 23ms/step - loss: 16.0180 - acc: 0.6230 - val_loss: 14.1204 - val_acc: 0.6359 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 22.3627 - acc: 0.6144\n",
      "Epoch 4: val_acc improved from 0.75728 to 0.78155, saving model to models/exer1\\classifier_acc.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 8.58111 to 7.87510, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 26ms/step - loss: 22.0300 - acc: 0.6160 - val_loss: 7.8751 - val_acc: 0.7816 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 4.6547 - acc: 0.7572\n",
      "Epoch 5: val_acc did not improve from 0.78155\n",
      "\n",
      "Epoch 5: val_loss improved from 7.87510 to 3.94549, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 4.6547 - acc: 0.7572 - val_loss: 3.9455 - val_acc: 0.7233 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 4.5765 - acc: 0.7522\n",
      "Epoch 6: val_acc improved from 0.78155 to 0.79612, saving model to models/exer1\\classifier_acc.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 3.94549 to 3.77113, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 4.5295 - acc: 0.7523 - val_loss: 3.7711 - val_acc: 0.7961 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 6.0708 - acc: 0.7054\n",
      "Epoch 7: val_acc improved from 0.79612 to 0.85922, saving model to models/exer1\\classifier_acc.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 3.77113 to 2.42546, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 17ms/step - loss: 6.0071 - acc: 0.7052 - val_loss: 2.4255 - val_acc: 0.8592 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 1.8372 - acc: 0.8574\n",
      "Epoch 8: val_acc improved from 0.85922 to 0.87379, saving model to models/exer1\\classifier_acc.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 2.42546 to 1.91852, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 18ms/step - loss: 1.7693 - acc: 0.8599 - val_loss: 1.9185 - val_acc: 0.8738 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 1.2327 - acc: 0.8690\n",
      "Epoch 9: val_acc improved from 0.87379 to 0.89320, saving model to models/exer1\\classifier_acc.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.91852 to 0.79079, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 17ms/step - loss: 1.2279 - acc: 0.8686 - val_loss: 0.7908 - val_acc: 0.8932 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 1.3510 - acc: 0.8865\n",
      "Epoch 10: val_acc did not improve from 0.89320\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.79079\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 1.3374 - acc: 0.8870 - val_loss: 1.3639 - val_acc: 0.8592 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.0900 - acc: 0.8778\n",
      "Epoch 11: val_acc did not improve from 0.89320\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.79079\n",
      "58/58 [==============================] - 1s 17ms/step - loss: 1.0900 - acc: 0.8778 - val_loss: 1.4826 - val_acc: 0.8155 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 1.5006 - acc: 0.8531\n",
      "Epoch 12: val_acc improved from 0.89320 to 0.98058, saving model to models/exer1\\classifier_acc.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 0.79079 to 0.59592, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 17ms/step - loss: 1.4833 - acc: 0.8534 - val_loss: 0.5959 - val_acc: 0.9806 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.1728 - acc: 0.9698\n",
      "Epoch 13: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.59592\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.1707 - acc: 0.9703 - val_loss: 1.2720 - val_acc: 0.9806 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.2885 - acc: 0.9636\n",
      "Epoch 14: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 14: val_loss improved from 0.59592 to 0.07951, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.2749 - acc: 0.9654 - val_loss: 0.0795 - val_acc: 0.9757 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.8915 - acc: 0.9091\n",
      "Epoch 15: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.8915 - acc: 0.9091 - val_loss: 0.1544 - val_acc: 0.9515 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.9924 - acc: 0.9051\n",
      "Epoch 16: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 1.0345 - acc: 0.8978 - val_loss: 1.0588 - val_acc: 0.8786 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 1.1450 - acc: 0.8772\n",
      "Epoch 17: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 1.1283 - acc: 0.8767 - val_loss: 0.2496 - val_acc: 0.9709 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3434 - acc: 0.9410\n",
      "Epoch 18: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.3386 - acc: 0.9410 - val_loss: 0.3839 - val_acc: 0.9417 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.6827 - acc: 0.8981\n",
      "Epoch 19: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.7060 - acc: 0.8956 - val_loss: 1.8200 - val_acc: 0.7718 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5290 - acc: 0.9216\n",
      "Epoch 20: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.5290 - acc: 0.9216 - val_loss: 1.0609 - val_acc: 0.8447 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3351 - acc: 0.9392\n",
      "Epoch 21: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.3419 - acc: 0.9356 - val_loss: 0.2794 - val_acc: 0.9320 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3357 - acc: 0.9381\n",
      "Epoch 22: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.3755 - acc: 0.9324 - val_loss: 0.1623 - val_acc: 0.9709 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.5526 - acc: 0.9085\n",
      "Epoch 23: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.5391 - acc: 0.9108 - val_loss: 0.3555 - val_acc: 0.9272 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.9386\n",
      "Epoch 24: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.3146 - acc: 0.9389 - val_loss: 0.3147 - val_acc: 0.9612 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.5991 - acc: 0.9018\n",
      "Epoch 25: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.6372 - acc: 0.8994 - val_loss: 0.2808 - val_acc: 0.9660 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3324 - acc: 0.9472\n",
      "Epoch 26: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.3242 - acc: 0.9475 - val_loss: 0.1686 - val_acc: 0.9757 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.2114 - acc: 0.9570\n",
      "Epoch 27: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.2342 - acc: 0.9540 - val_loss: 0.6840 - val_acc: 0.9029 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.9391\n",
      "Epoch 28: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.2705 - acc: 0.9400 - val_loss: 0.6981 - val_acc: 0.8835 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.4784 - acc: 0.9167\n",
      "Epoch 29: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.5103 - acc: 0.9156 - val_loss: 2.0806 - val_acc: 0.7670 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 1.5551 - acc: 0.8633\n",
      "Epoch 30: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 1.5197 - acc: 0.8653 - val_loss: 0.6668 - val_acc: 0.9272 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.7148 - acc: 0.9037\n",
      "Epoch 31: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.7148 - acc: 0.9037 - val_loss: 0.4164 - val_acc: 0.9466 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.8616 - acc: 0.8929\n",
      "Epoch 32: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.8663 - acc: 0.8935 - val_loss: 0.8381 - val_acc: 0.9029 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3799 - acc: 0.9457\n",
      "Epoch 33: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.3787 - acc: 0.9454 - val_loss: 0.2093 - val_acc: 0.9757 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.2659 - acc: 0.9557\n",
      "Epoch 34: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.2681 - acc: 0.9551 - val_loss: 0.2962 - val_acc: 0.9612 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3744 - acc: 0.9364\n",
      "Epoch 35: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.3775 - acc: 0.9362 - val_loss: 0.3423 - val_acc: 0.9515 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 1.1121 - acc: 0.8722\n",
      "Epoch 36: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 1.0907 - acc: 0.8745 - val_loss: 3.2882 - val_acc: 0.6990 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 1.1314 - acc: 0.8915\n",
      "Epoch 37: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 1.0845 - acc: 0.8956 - val_loss: 0.2381 - val_acc: 0.9709 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3419 - acc: 0.9442\n",
      "Epoch 38: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.3340 - acc: 0.9448 - val_loss: 0.2145 - val_acc: 0.9757 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3742 - acc: 0.9438\n",
      "Epoch 39: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.3805 - acc: 0.9438 - val_loss: 0.2169 - val_acc: 0.9466 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.5698 - acc: 0.9189\n",
      "Epoch 40: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.5621 - acc: 0.9200 - val_loss: 0.7630 - val_acc: 0.8981 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3106 - acc: 0.9420\n",
      "Epoch 41: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.2985 - acc: 0.9438 - val_loss: 0.6078 - val_acc: 0.9223 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3702 - acc: 0.9375\n",
      "Epoch 42: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.3553 - acc: 0.9394 - val_loss: 0.2536 - val_acc: 0.9515 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.6027 - acc: 0.9199\n",
      "Epoch 43: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.5803 - acc: 0.9227 - val_loss: 0.1258 - val_acc: 0.9806 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.1981 - acc: 0.9644\n",
      "Epoch 44: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.1983 - acc: 0.9643 - val_loss: 0.2150 - val_acc: 0.9417 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2341 - acc: 0.9567\n",
      "Epoch 45: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.2341 - acc: 0.9567 - val_loss: 0.2347 - val_acc: 0.9612 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.5774 - acc: 0.9301\n",
      "Epoch 46: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.5839 - acc: 0.9281 - val_loss: 0.7971 - val_acc: 0.8932 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 1.1765 - acc: 0.8811\n",
      "Epoch 47: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 1.2039 - acc: 0.8772 - val_loss: 0.4190 - val_acc: 0.9563 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.5185 - acc: 0.9402\n",
      "Epoch 48: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.5115 - acc: 0.9410 - val_loss: 0.2120 - val_acc: 0.9806 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3344 - acc: 0.9519\n",
      "Epoch 49: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.3344 - acc: 0.9519 - val_loss: 0.7591 - val_acc: 0.9175 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.5988 - acc: 0.9247\n",
      "Epoch 50: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.07951\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.5923 - acc: 0.9259 - val_loss: 0.9322 - val_acc: 0.8738 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3401 - acc: 0.9502\n",
      "Epoch 51: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 51: val_loss improved from 0.07951 to 0.04843, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.3242 - acc: 0.9519 - val_loss: 0.0484 - val_acc: 0.9806 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.2546 - acc: 0.9593\n",
      "Epoch 52: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.04843\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.2522 - acc: 0.9594 - val_loss: 0.1273 - val_acc: 0.9709 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3633 - acc: 0.9410\n",
      "Epoch 53: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.04843\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.3623 - acc: 0.9427 - val_loss: 1.2925 - val_acc: 0.8301 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.9704\n",
      "Epoch 54: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.04843\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.1589 - acc: 0.9692 - val_loss: 0.4031 - val_acc: 0.9660 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.4454 - acc: 0.9420\n",
      "Epoch 55: val_acc did not improve from 0.98058\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.04843\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.4333 - acc: 0.9432 - val_loss: 0.0634 - val_acc: 0.9806 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.2310 - acc: 0.9598\n",
      "Epoch 56: val_acc improved from 0.98058 to 0.98544, saving model to models/exer1\\classifier_acc.h5\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.04843\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.2386 - acc: 0.9584 - val_loss: 0.1149 - val_acc: 0.9854 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1396 - acc: 0.9708\n",
      "Epoch 57: val_acc improved from 0.98544 to 0.99029, saving model to models/exer1\\classifier_acc.h5\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.04843\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.1396 - acc: 0.9708 - val_loss: 0.0885 - val_acc: 0.9903 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3872 - acc: 0.9452\n",
      "Epoch 58: val_acc improved from 0.99029 to 0.99515, saving model to models/exer1\\classifier_acc.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.04843 to 0.02049, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.4026 - acc: 0.9438 - val_loss: 0.0205 - val_acc: 0.9951 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.1837 - acc: 0.9705\n",
      "Epoch 59: val_acc did not improve from 0.99515\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.02049\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.1743 - acc: 0.9713 - val_loss: 0.1136 - val_acc: 0.9806 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.1045 - acc: 0.9754\n",
      "Epoch 60: val_acc did not improve from 0.99515\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.02049\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.1128 - acc: 0.9757 - val_loss: 0.4060 - val_acc: 0.9320 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3351 - acc: 0.9531\n",
      "Epoch 61: val_acc did not improve from 0.99515\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.02049\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.3303 - acc: 0.9546 - val_loss: 0.5099 - val_acc: 0.9466 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.6367 - acc: 0.9309\n",
      "Epoch 62: val_acc did not improve from 0.99515\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.02049\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.6374 - acc: 0.9313 - val_loss: 0.2020 - val_acc: 0.9757 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3081 - acc: 0.9621\n",
      "Epoch 63: val_acc did not improve from 0.99515\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.02049\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.3081 - acc: 0.9621 - val_loss: 0.0513 - val_acc: 0.9854 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.1280 - acc: 0.9830\n",
      "Epoch 64: val_acc improved from 0.99515 to 1.00000, saving model to models/exer1\\classifier_acc.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.02049 to 0.00687, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.1225 - acc: 0.9832 - val_loss: 0.0069 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9857\n",
      "Epoch 65: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00687\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0743 - acc: 0.9859 - val_loss: 2.0049 - val_acc: 0.7816 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3468 - acc: 0.9568\n",
      "Epoch 66: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00687\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.3332 - acc: 0.9578 - val_loss: 0.8159 - val_acc: 0.9126 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.9863\n",
      "Epoch 67: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 67: val_loss improved from 0.00687 to 0.00009, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.1236 - acc: 0.9859 - val_loss: 8.6833e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.5163 - acc: 0.9352\n",
      "Epoch 68: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.5028 - acc: 0.9356 - val_loss: 0.2000 - val_acc: 0.9563 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.2122 - acc: 0.9561\n",
      "Epoch 69: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.2122 - acc: 0.9562 - val_loss: 0.1216 - val_acc: 0.9709 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.1529 - acc: 0.9676\n",
      "Epoch 70: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.1563 - acc: 0.9681 - val_loss: 0.1656 - val_acc: 0.9612 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.1144 - acc: 0.9816\n",
      "Epoch 71: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.1157 - acc: 0.9816 - val_loss: 0.1161 - val_acc: 0.9806 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0862 - acc: 0.9881\n",
      "Epoch 72: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0927 - acc: 0.9870 - val_loss: 4.2521e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.7059 - acc: 0.9323\n",
      "Epoch 73: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.6695 - acc: 0.9351 - val_loss: 0.3228 - val_acc: 0.9854 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.2873 - acc: 0.9671\n",
      "Epoch 74: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.3074 - acc: 0.9670 - val_loss: 0.0017 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 1.4679 - acc: 0.9124\n",
      "Epoch 75: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 1.4749 - acc: 0.9113 - val_loss: 0.6158 - val_acc: 0.9417 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.6743 - acc: 0.9286\n",
      "Epoch 76: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.6538 - acc: 0.9308 - val_loss: 0.7018 - val_acc: 0.8689 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 47.0511 - acc: 0.8011\n",
      "Epoch 77: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 45.3786 - acc: 0.7918 - val_loss: 1.0695 - val_acc: 0.8835 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 1.7987 - acc: 0.8817\n",
      "Epoch 78: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 1.7669 - acc: 0.8843 - val_loss: 0.3381 - val_acc: 0.9660 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3900 - acc: 0.9583\n",
      "Epoch 79: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.3888 - acc: 0.9584 - val_loss: 0.3608 - val_acc: 0.9320 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4278 - acc: 0.9556\n",
      "Epoch 80: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.4323 - acc: 0.9551 - val_loss: 0.1676 - val_acc: 0.9806 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.1160 - acc: 0.9794\n",
      "Epoch 81: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.1194 - acc: 0.9778 - val_loss: 0.1093 - val_acc: 0.9806 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2805 - acc: 0.9676\n",
      "Epoch 82: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.2805 - acc: 0.9676 - val_loss: 0.8265 - val_acc: 0.8883 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.6308 - acc: 0.9426\n",
      "Epoch 83: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.6623 - acc: 0.9400 - val_loss: 2.0273 - val_acc: 0.8738 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.5605 - acc: 0.9526\n",
      "Epoch 84: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.5435 - acc: 0.9540 - val_loss: 0.0093 - val_acc: 0.9951 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0552 - acc: 0.9897\n",
      "Epoch 85: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0552 - acc: 0.9897 - val_loss: 0.1283 - val_acc: 0.9854 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9852\n",
      "Epoch 86: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0871 - acc: 0.9854 - val_loss: 1.4621e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9945\n",
      "Epoch 87: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0304 - acc: 0.9941 - val_loss: 1.1053e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0430 - acc: 0.9942\n",
      "Epoch 88: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0403 - acc: 0.9946 - val_loss: 1.7421e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0131 - acc: 0.9965\n",
      "Epoch 89: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0319 - acc: 0.9957 - val_loss: 8.8084e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0529 - acc: 0.9896\n",
      "Epoch 90: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0515 - acc: 0.9897 - val_loss: 0.0333 - val_acc: 0.9951 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0634 - acc: 0.9898\n",
      "Epoch 91: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0672 - acc: 0.9892 - val_loss: 0.2136 - val_acc: 0.9854 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1577 - acc: 0.9789\n",
      "Epoch 92: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.1577 - acc: 0.9789 - val_loss: 0.0481 - val_acc: 0.9951 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.1586 - acc: 0.9810\n",
      "Epoch 93: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.1537 - acc: 0.9816 - val_loss: 0.0031 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9885\n",
      "Epoch 94: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0708 - acc: 0.9886 - val_loss: 0.0095 - val_acc: 0.9951 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0522 - acc: 0.9919\n",
      "Epoch 95: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0504 - acc: 0.9919 - val_loss: 0.0273 - val_acc: 0.9951 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0585 - acc: 0.9909\n",
      "Epoch 96: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0599 - acc: 0.9892 - val_loss: 9.7660e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0438 - acc: 0.9946\n",
      "Epoch 97: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0438 - acc: 0.9946 - val_loss: 0.0049 - val_acc: 0.9951 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9956\n",
      "Epoch 98: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0530 - acc: 0.9951 - val_loss: 9.6276e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0606 - acc: 0.9897\n",
      "Epoch 99: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0606 - acc: 0.9897 - val_loss: 0.0015 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0137 - acc: 0.9955\n",
      "Epoch 100: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0133 - acc: 0.9957 - val_loss: 1.1306e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1223 - acc: 0.9876\n",
      "Epoch 101: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00009\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.1223 - acc: 0.9876 - val_loss: 0.0174 - val_acc: 0.9951 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4967 - acc: 0.9951\n",
      "Epoch 102: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 102: val_loss improved from 0.00009 to 0.00001, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.4900 - acc: 0.9951 - val_loss: 1.1228e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.1729 - acc: 0.9940\n",
      "Epoch 103: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.1706 - acc: 0.9941 - val_loss: 0.3966 - val_acc: 0.9951 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 1.0738 - acc: 0.9353\n",
      "Epoch 104: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 1.0735 - acc: 0.9351 - val_loss: 1.4317 - val_acc: 0.8932 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5432 - acc: 0.9346\n",
      "Epoch 105: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.5432 - acc: 0.9346 - val_loss: 1.1253 - val_acc: 0.9320 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 1.7315 - acc: 0.8929\n",
      "Epoch 106: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 1.6457 - acc: 0.8972 - val_loss: 0.9416 - val_acc: 0.9029 - lr: 0.0010\n",
      "Epoch 107/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 3.9535 - acc: 0.7992\n",
      "Epoch 107: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 4.1288 - acc: 0.7988 - val_loss: 1.1193 - val_acc: 0.8689 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 4.3635 - acc: 0.7517\n",
      "Epoch 108: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 4.2921 - acc: 0.7550 - val_loss: 0.5234 - val_acc: 0.9417 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 1.6271 - acc: 0.9011\n",
      "Epoch 109: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 1.5751 - acc: 0.9043 - val_loss: 0.0960 - val_acc: 0.9854 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3345 - acc: 0.9704\n",
      "Epoch 110: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.3313 - acc: 0.9708 - val_loss: 2.8219e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 111/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9879\n",
      "Epoch 111: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0867 - acc: 0.9881 - val_loss: 0.0313 - val_acc: 0.9903 - lr: 0.0010\n",
      "Epoch 112/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.1015 - acc: 0.9916\n",
      "Epoch 112: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0983 - acc: 0.9919 - val_loss: 0.0377 - val_acc: 0.9951 - lr: 0.0010\n",
      "Epoch 113/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0793 - acc: 0.9897\n",
      "Epoch 113: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0793 - acc: 0.9897 - val_loss: 2.2240e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0484 - acc: 0.9949\n",
      "Epoch 114: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00001\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0461 - acc: 0.9951 - val_loss: 3.3064e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0133 - acc: 0.9977\n",
      "Epoch 115: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0124 - acc: 0.9978 - val_loss: 5.9003e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 116/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9978\n",
      "Epoch 116: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0083 - acc: 0.9978 - val_loss: 1.2402e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 117/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9978\n",
      "Epoch 117: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 117: val_loss improved from 0.00001 to 0.00001, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0070 - acc: 0.9978 - val_loss: 8.2792e-06 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 118/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9973\n",
      "Epoch 118: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0076 - acc: 0.9973 - val_loss: 1.0820e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 119/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9978\n",
      "Epoch 119: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0070 - acc: 0.9978 - val_loss: 9.8648e-06 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 120/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9978\n",
      "Epoch 120: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0066 - acc: 0.9978 - val_loss: 2.2957e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 121/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0061 - acc: 0.9978\n",
      "Epoch 121: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0059 - acc: 0.9978 - val_loss: 1.1217e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 122/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0087 - acc: 0.9978\n",
      "Epoch 122: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0087 - acc: 0.9978 - val_loss: 9.7491e-06 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 123/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9984\n",
      "Epoch 123: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0063 - acc: 0.9984 - val_loss: 6.2915e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 124/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9984\n",
      "Epoch 124: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0079 - acc: 0.9984 - val_loss: 7.8773e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 125/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0063 - acc: 0.9983\n",
      "Epoch 125: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0072 - acc: 0.9978 - val_loss: 2.4253e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 126/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0067 - acc: 0.9983\n",
      "Epoch 126: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0063 - acc: 0.9984 - val_loss: 2.4294e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 127/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9983\n",
      "Epoch 127: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0066 - acc: 0.9978 - val_loss: 1.4406e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 128/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0078 - acc: 0.9977\n",
      "Epoch 128: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0074 - acc: 0.9978 - val_loss: 1.0220e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 129/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9984\n",
      "Epoch 129: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0057 - acc: 0.9984 - val_loss: 2.3844e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 130/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0058 - acc: 0.9983 \n",
      "Epoch 130: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0057 - acc: 0.9984 - val_loss: 2.1863e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 131/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9978\n",
      "Epoch 131: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0062 - acc: 0.9978 - val_loss: 1.0954e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 132/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0075 - acc: 0.9977\n",
      "Epoch 132: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0070 - acc: 0.9978 - val_loss: 2.7851e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 133/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9978\n",
      "Epoch 133: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0068 - acc: 0.9978 - val_loss: 9.5368e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 134/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9984 \n",
      "Epoch 134: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0069 - acc: 0.9984 - val_loss: 1.1543e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 135/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9989\n",
      "Epoch 135: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0060 - acc: 0.9984 - val_loss: 9.6475e-06 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 136/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9984 \n",
      "Epoch 136: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0054 - acc: 0.9984 - val_loss: 1.5935e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 137/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9989\n",
      "Epoch 137: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0058 - acc: 0.9984 - val_loss: 2.0962e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 138/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0059 - acc: 0.9983 \n",
      "Epoch 138: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 138: val_loss improved from 0.00001 to 0.00001, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0061 - acc: 0.9984 - val_loss: 6.9921e-06 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 139/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0063 - acc: 0.9977\n",
      "Epoch 139: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0060 - acc: 0.9978 - val_loss: 3.1960e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 140/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0078 - acc: 0.9983\n",
      "Epoch 140: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0073 - acc: 0.9984 - val_loss: 7.7778e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 141/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0069 - acc: 0.9983\n",
      "Epoch 141: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0065 - acc: 0.9984 - val_loss: 4.3820e-04 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 142/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9984\n",
      "Epoch 142: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0066 - acc: 0.9984 - val_loss: 3.7381e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 143/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0070 - acc: 0.9983\n",
      "Epoch 143: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0065 - acc: 0.9984 - val_loss: 7.3340e-06 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 144/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0042 - acc: 0.9988\n",
      "Epoch 144: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 144: val_loss improved from 0.00001 to 0.00001, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0059 - acc: 0.9984 - val_loss: 6.2639e-06 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 145/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0078 - acc: 0.9983\n",
      "Epoch 145: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0075 - acc: 0.9984 - val_loss: 8.4636e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 146/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0044 - acc: 0.9989\n",
      "Epoch 146: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0042 - acc: 0.9989 - val_loss: 2.9526e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 147/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0059 - acc: 0.9983\n",
      "Epoch 147: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0056 - acc: 0.9984 - val_loss: 1.5735e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 148/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0068 - acc: 0.9983\n",
      "Epoch 148: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 148: val_loss improved from 0.00001 to 0.00001, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0066 - acc: 0.9984 - val_loss: 6.0884e-06 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 149/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0037 - acc: 0.9984\n",
      "Epoch 149: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0037 - acc: 0.9984 - val_loss: 1.3034e-04 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 150/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9978\n",
      "Epoch 150: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 150: val_loss improved from 0.00001 to 0.00001, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0066 - acc: 0.9978 - val_loss: 5.4451e-06 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 151/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9967\n",
      "Epoch 151: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00001\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0068 - acc: 0.9968 - val_loss: 0.0025 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 152/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0064 - acc: 0.9971\n",
      "Epoch 152: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 152: val_loss improved from 0.00001 to 0.00000, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0080 - acc: 0.9968 - val_loss: 4.5235e-06 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 153/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9978\n",
      "Epoch 153: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0077 - acc: 0.9978 - val_loss: 3.0977e-04 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 154/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0054 - acc: 0.9983\n",
      "Epoch 154: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0053 - acc: 0.9984 - val_loss: 1.6650e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 155/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0057 - acc: 0.9983\n",
      "Epoch 155: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0053 - acc: 0.9984 - val_loss: 9.4925e-06 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 156/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0060 - acc: 0.9983\n",
      "Epoch 156: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 156: val_loss improved from 0.00000 to 0.00000, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0056 - acc: 0.9984 - val_loss: 3.3523e-06 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 157/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0047 - acc: 0.9983\n",
      "Epoch 157: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0044 - acc: 0.9984 - val_loss: 3.5223e-06 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 158/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0056 - acc: 0.9983\n",
      "Epoch 158: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 158: val_loss improved from 0.00000 to 0.00000, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0054 - acc: 0.9984 - val_loss: 3.3364e-06 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 159/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9983\n",
      "Epoch 159: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0034 - acc: 0.9984 - val_loss: 7.1933e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 160/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9989\n",
      "Epoch 160: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 0.0017 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 161/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9978\n",
      "Epoch 161: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0039 - acc: 0.9978 - val_loss: 8.4140e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 162/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9984\n",
      "Epoch 162: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0077 - acc: 0.9984 - val_loss: 1.3447e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 163/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9978\n",
      "Epoch 163: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0059 - acc: 0.9978 - val_loss: 2.4200e-05 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 164/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0060 - acc: 0.9983\n",
      "Epoch 164: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 164: val_loss improved from 0.00000 to 0.00000, saving model to models/exer1\\classifier_loss.h5\n",
      "\n",
      "Epoch 164: ReduceLROnPlateau reducing learning rate to 2.5000001187436284e-06.\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0058 - acc: 0.9984 - val_loss: 3.2619e-06 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 165/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9984\n",
      "Epoch 165: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 165: val_loss improved from 0.00000 to 0.00000, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0027 - acc: 0.9984 - val_loss: 2.8796e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 166/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0027 - acc: 0.9989\n",
      "Epoch 166: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 166: val_loss improved from 0.00000 to 0.00000, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0027 - acc: 0.9989 - val_loss: 2.8172e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 167/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0028 - acc: 0.9988  \n",
      "Epoch 167: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 167: val_loss improved from 0.00000 to 0.00000, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0026 - acc: 0.9989 - val_loss: 2.8149e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 168/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9989\n",
      "Epoch 168: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0026 - acc: 0.9989 - val_loss: 2.9156e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 169/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9989\n",
      "Epoch 169: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 3.0925e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 170/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9994 \n",
      "Epoch 170: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0026 - acc: 0.9989 - val_loss: 3.9751e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 171/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0028 - acc: 0.9988\n",
      "Epoch 171: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0026 - acc: 0.9989 - val_loss: 2.9578e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 172/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0025 - acc: 0.9989\n",
      "Epoch 172: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 2.9988e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 173/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9989\n",
      "Epoch 173: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 3.0983e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 174/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9989\n",
      "Epoch 174: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0026 - acc: 0.9989 - val_loss: 3.4274e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 175/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 175: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0026 - acc: 0.9989 - val_loss: 4.2370e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 176/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9989\n",
      "Epoch 176: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 176: val_loss improved from 0.00000 to 0.00000, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0024 - acc: 0.9989 - val_loss: 2.7848e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 177/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9989\n",
      "Epoch 177: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0026 - acc: 0.9989 - val_loss: 2.8236e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 178/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9989\n",
      "Epoch 178: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0024 - acc: 0.9989 - val_loss: 3.1394e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 179/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0027 - acc: 0.9988\n",
      "Epoch 179: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 3.1463e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 180/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0028 - acc: 0.9988\n",
      "Epoch 180: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0026 - acc: 0.9989 - val_loss: 3.0404e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 181/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9989\n",
      "Epoch 181: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 3.0589e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 182/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9989\n",
      "Epoch 182: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0024 - acc: 0.9989 - val_loss: 3.6957e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 183/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9989\n",
      "Epoch 183: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0026 - acc: 0.9989 - val_loss: 3.8570e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 184/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9989\n",
      "Epoch 184: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 3.2157e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 185/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9989\n",
      "Epoch 185: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 185: val_loss improved from 0.00000 to 0.00000, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 2.7200e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 186/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9989\n",
      "Epoch 186: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0024 - acc: 0.9989 - val_loss: 3.2185e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 187/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9989\n",
      "Epoch 187: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0024 - acc: 0.9989 - val_loss: 3.4475e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 188/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9989\n",
      "Epoch 188: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 3.2989e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 189/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0027 - acc: 0.9988\n",
      "Epoch 189: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 3.8136e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 190/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0027 - acc: 0.9989\n",
      "Epoch 190: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 190: val_loss improved from 0.00000 to 0.00000, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0027 - acc: 0.9989 - val_loss: 2.6621e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 191/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0025 - acc: 0.9989\n",
      "Epoch 191: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 3.4024e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 192/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9989\n",
      "Epoch 192: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0027 - acc: 0.9989 - val_loss: 2.9334e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 193/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9989\n",
      "Epoch 193: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0024 - acc: 0.9989 - val_loss: 3.0959e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 194/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9989   \n",
      "Epoch 194: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0023 - acc: 0.9989 - val_loss: 2.8483e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 195/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0026 - acc: 0.9988\n",
      "Epoch 195: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 3.3931e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 196/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9989\n",
      "Epoch 196: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 4.8017e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 197/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9989\n",
      "Epoch 197: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 197: val_loss improved from 0.00000 to 0.00000, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0023 - acc: 0.9989 - val_loss: 2.4955e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 198/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9989\n",
      "Epoch 198: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 198: val_loss improved from 0.00000 to 0.00000, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 2.2439e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 199/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9989\n",
      "Epoch 199: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 2.4990e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 200/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9989\n",
      "Epoch 200: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0024 - acc: 0.9989 - val_loss: 2.8009e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 201/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9989 \n",
      "Epoch 201: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0024 - acc: 0.9989 - val_loss: 6.0520e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 202/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9989\n",
      "Epoch 202: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 2.6684e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 203/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9989\n",
      "Epoch 203: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 2.4365e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 204/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9989\n",
      "Epoch 204: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0023 - acc: 0.9989 - val_loss: 2.6025e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 205/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9989\n",
      "Epoch 205: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0023 - acc: 0.9989 - val_loss: 2.9744e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 206/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9989\n",
      "Epoch 206: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 17ms/step - loss: 0.0022 - acc: 0.9989 - val_loss: 3.0605e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 207/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9989\n",
      "Epoch 207: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0024 - acc: 0.9989 - val_loss: 3.0009e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 208/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9989\n",
      "Epoch 208: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 208: val_loss improved from 0.00000 to 0.00000, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0023 - acc: 0.9989 - val_loss: 2.2149e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 209/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9989\n",
      "Epoch 209: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0021 - acc: 0.9989 - val_loss: 2.4023e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 210/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 210: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0020 - acc: 0.9989 - val_loss: 2.4821e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 211/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9989\n",
      "Epoch 211: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0023 - acc: 0.9989 - val_loss: 4.2338e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 212/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9989\n",
      "Epoch 212: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 212: val_loss improved from 0.00000 to 0.00000, saving model to models/exer1\\classifier_loss.h5\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0022 - acc: 0.9989 - val_loss: 1.8701e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 213/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9989\n",
      "Epoch 213: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0024 - acc: 0.9989 - val_loss: 2.4758e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 214/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9989\n",
      "Epoch 214: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 214: ReduceLROnPlateau reducing learning rate to 1.2500000821091816e-07.\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0022 - acc: 0.9989 - val_loss: 1.9546e-06 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 215/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 215: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0020 - acc: 0.9989 - val_loss: 1.9442e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 216/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9988\n",
      "Epoch 216: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0020 - acc: 0.9989 - val_loss: 1.9586e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 217/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9989 \n",
      "Epoch 217: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0020 - acc: 0.9989 - val_loss: 2.0008e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 218/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 218: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0020 - acc: 0.9989 - val_loss: 2.0350e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 219/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9988\n",
      "Epoch 219: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0020 - acc: 0.9989 - val_loss: 2.0072e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 220/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 220: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0020 - acc: 0.9989 - val_loss: 2.0298e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 221/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9989    \n",
      "Epoch 221: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0020 - acc: 0.9989 - val_loss: 2.0194e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 222/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9989\n",
      "Epoch 222: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0020 - acc: 0.9989 - val_loss: 2.0685e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 223/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9989\n",
      "Epoch 223: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0020 - acc: 0.9989 - val_loss: 2.1009e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 224/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 224: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.0969e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 225/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 225: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0020 - acc: 0.9989 - val_loss: 2.1593e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 226/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 226: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0020 - acc: 0.9989 - val_loss: 2.1264e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 227/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 227: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0020 - acc: 0.9989 - val_loss: 2.1391e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 228/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9989    \n",
      "Epoch 228: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.1096e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 229/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 229: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0020 - acc: 0.9989 - val_loss: 2.1993e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 230/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989    \n",
      "Epoch 230: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.1738e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 231/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 231: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.2478e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 232/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 232: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.2045e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 233/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 233: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.2264e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 234/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 234: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.2230e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 235/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 235: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.2449e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 236/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 236: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0020 - acc: 0.9989 - val_loss: 2.2825e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 237/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 237: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.3005e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 238/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 238: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.2594e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 239/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 239: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.2860e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 240/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 240: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.2860e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 241/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 241: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.2831e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 242/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989    \n",
      "Epoch 242: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.2322e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 243/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 243: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.3323e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 244/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 244: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.3173e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 245/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 9.3648e-04 - acc: 0.9994\n",
      "Epoch 245: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.2426e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 246/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9988    \n",
      "Epoch 246: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.2808e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 247/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 247: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.3479e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 248/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 248: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.3635e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 249/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 249: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4277e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 250/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 250: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.3583e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 251/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9989    \n",
      "Epoch 251: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.3045e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 252/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 252: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.3872e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 253/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 253: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.3988e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 254/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 254: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.3791e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 255/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 255: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.3525e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 256/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 256: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.3959e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 257/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 257: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4838e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 258/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 258: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.3942e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 259/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9988\n",
      "Epoch 259: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4069e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 260/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989    \n",
      "Epoch 260: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4133e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 261/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 261: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4347e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 262/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 262: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4289e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 263/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9989   \n",
      "Epoch 263: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.3485e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 264/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 264: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 264: ReduceLROnPlateau reducing learning rate to 6.250000694763003e-09.\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4780e-06 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 265/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 265: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4769e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 266/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 266: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4775e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 267/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 267: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4769e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 268/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0018 - acc: 0.9989\n",
      "Epoch 268: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4775e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 269/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 269: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4769e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 270/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 270: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4769e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 271/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 271: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4775e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 272/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989    \n",
      "Epoch 272: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4775e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 273/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989 \n",
      "Epoch 273: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4792e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 274/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 274: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4775e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 275/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 275: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4775e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 276/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 276: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4775e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 277/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 277: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4775e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 278/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 278: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4769e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 279/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 279: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4757e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 280/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 280: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4746e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 281/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 281: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4757e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 282/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 282: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4752e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 283/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 283: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4763e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 284/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 284: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4763e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 285/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 285: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4769e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 286/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 286: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4769e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 287/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 287: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4786e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 288/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989 \n",
      "Epoch 288: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4757e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 289/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 289: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4752e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 290/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 290: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4752e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 291/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 291: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4752e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 292/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 292: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4752e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 293/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 293: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4763e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 294/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 294: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4752e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 295/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 295: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4746e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 296/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 296: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4734e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 297/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 297: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 298/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989    \n",
      "Epoch 298: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4711e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 299/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 299: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4717e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 300/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 300: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4711e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 301/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 301: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 301: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 302/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 302: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 302: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4711e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 303/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 303: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 303: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4717e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 304/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 304: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 304: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 305/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 305: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 305: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4711e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 306/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9988\n",
      "Epoch 306: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 306: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 307/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 307: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 307: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 308/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 308: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 308: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 309/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989    \n",
      "Epoch 309: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 309: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4717e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 310/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9988\n",
      "Epoch 310: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 310: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4746e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 311/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 311: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 311: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4746e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 312/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 312: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 312: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4746e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 313/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 313: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 313: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 314/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 314: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 314: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 314: ReduceLROnPlateau reducing learning rate to 3.1250002585636594e-10.\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 315/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 315: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 315: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 316/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 316: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 316: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 317/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 317: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 317: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 318/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 318: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 318: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 319/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 319: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 319: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 320/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 320: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 320: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 321/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 321: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 321: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 322/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 322: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 322: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 323/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 323: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 323: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 324/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 324: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 324: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 325/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9988\n",
      "Epoch 325: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 325: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 326/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 326: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 326: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 327/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 327: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 327: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 328/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989  \n",
      "Epoch 328: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 328: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 329/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 329: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 329: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4734e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 330/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 330: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 330: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4734e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 331/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 331: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 331: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4734e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 332/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 332: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 332: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4734e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 333/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 333: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 333: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4734e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 334/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 334: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 334: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4734e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 335/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 335: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 335: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4734e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 336/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 336: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 336: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4734e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 337/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 337: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 337: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4734e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 338/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9988\n",
      "Epoch 338: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 338: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4734e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 339/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 339: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 339: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4734e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 340/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 340: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 340: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 341/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9988\n",
      "Epoch 341: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 341: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 342/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 342: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 342: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 343/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 343: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 343: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 344/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 344: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 344: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 345/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989    \n",
      "Epoch 345: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 345: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4740e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 346/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 346: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 346: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4734e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 347/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 347: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 347: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4734e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 348/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 348: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 348: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 349/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 349: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 349: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 350/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 350: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 350: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 351/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 351: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 351: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 352/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 352: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 352: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 353/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.0020 - acc: 0.9988\n",
      "Epoch 353: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 353: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 354/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 354: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 354: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 355/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 355: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 355: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4734e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 356/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 356: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 356: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 357/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 357: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 357: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 358/1000\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 9.8003e-04 - acc: 0.9994\n",
      "Epoch 358: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 358: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4734e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 359/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989    \n",
      "Epoch 359: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 359: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4734e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 360/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989 \n",
      "Epoch 360: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 360: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4734e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 361/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9995   \n",
      "Epoch 361: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 361: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4734e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 362/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 362: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 362: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 363/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 363: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 363: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4734e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 364/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 364: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 364: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 364: ReduceLROnPlateau reducing learning rate to 1.5625001292818297e-11.\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 3.1250e-10\n",
      "Epoch 365/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 365: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 365: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 366/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 366: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 366: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 367/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989 \n",
      "Epoch 367: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 367: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 368/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 368: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 368: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 369/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 369: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 369: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 370/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 370: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 370: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 371/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 371: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 371: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 372/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 372: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 372: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 373/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 373: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 373: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 374/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 374: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 374: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 375/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 375: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 375: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 376/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989 \n",
      "Epoch 376: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 376: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 377/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 377: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 377: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 378/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 378: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 378: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 379/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 379: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 379: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 380/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 380: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 380: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 381/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 381: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 381: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 382/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 382: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 382: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 383/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 383: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 383: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 384/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989    \n",
      "Epoch 384: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 384: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 14ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 385/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 385: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 385: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 386/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 386: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 386: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 387/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 387: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 387: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 388/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 388: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 388: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 389/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 389: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 389: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 390/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989 \n",
      "Epoch 390: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 390: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 391/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 391: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 391: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 392/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 392: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 392: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 393/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 393: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 393: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 394/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 394: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 394: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 395/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 395: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 395: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 396/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 396: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 396: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 397/1000\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9989\n",
      "Epoch 397: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 397: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 398/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 398: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 398: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 399/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 399: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 399: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 400/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 400: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 400: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 401/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 401: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 401: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 402/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989 \n",
      "Epoch 402: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 402: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 403/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 403: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 403: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 404/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 404: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 404: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 405/1000\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9989 \n",
      "Epoch 405: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 405: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 406/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 406: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 406: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 407/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989   \n",
      "Epoch 407: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 407: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 408/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 408: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 408: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 409/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989 \n",
      "Epoch 409: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 409: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 410/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 410: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 410: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 411/1000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 411: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 411: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n",
      "Epoch 412/1000\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9989\n",
      "Epoch 412: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 412: val_loss did not improve from 0.00000\n",
      "58/58 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 0.9989 - val_loss: 2.4728e-06 - val_acc: 1.0000 - lr: 1.5625e-11\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_tr, y_tr,\n",
    "    validation_data = (x_val, y_val),\n",
    "    epochs = 1000,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(f'models/{exercise}/classifier_acc.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "        ModelCheckpoint(f'models/{exercise}/classifier_loss.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto'),\n",
    "        ReduceLROnPlateau(monitor='val_acc', factor=0.05, patience=50, verbose=1, mode='auto'),\n",
    "        EarlyStopping(monitor='val_loss', min_delta=0, patience=200, mode='auto')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, loss_ax = plt.subplots(figsize=(16,10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'],'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['acc'],'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'],'g',label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
