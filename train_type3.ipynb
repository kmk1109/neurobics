{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROUTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise = 'exer2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('dataset/exer2/raw/raw_five_1670081688.npy'),\n",
       " WindowsPath('dataset/exer2/raw/raw_four_1670081688.npy'),\n",
       " WindowsPath('dataset/exer2/raw/raw_one_1670081688.npy'),\n",
       " WindowsPath('dataset/exer2/raw/raw_three_1670081688.npy'),\n",
       " WindowsPath('dataset/exer2/raw/raw_two_1670081688.npy')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_set_group = sorted([x for x in Path(f\"./dataset/{exercise}/raw/\").glob(\"*.npy\")])\n",
    "tr_set_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['five', 'four', 'three', 'two', 'one']\n"
     ]
    }
   ],
   "source": [
    "if exercise == 'exer1':\n",
    "    actions = ['thumb','little']\n",
    "elif exercise == 'exer2':\n",
    "    actions = ['five', 'four', 'three', 'two', 'one']\n",
    "else:\n",
    "    actions = ['thumb','paper']\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for fle in tr_set_group:\n",
    "    if len(data) == 0:\n",
    "        data = np.load(fle)\n",
    "    else:\n",
    "        data = np.concatenate([data, np.load(fle)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2746, 100)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2746, 99)\n",
      "(2746,)\n",
      "[0. 0. 0. ... 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "x_data = data[:,:-1]\n",
    "labels = data[:,-1]\n",
    "\n",
    "print(x_data.shape)\n",
    "print(labels.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2746, 5)\n"
     ]
    }
   ],
   "source": [
    "y_data = to_categorical(labels, num_classes=len(actions))\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2471, 99) (2471, 5)\n",
      "(275, 99) (275, 5)\n"
     ]
    }
   ],
   "source": [
    "x_data = x_data.astype(np.float32)\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "x_tr,x_val,y_tr,y_val = train_test_split(x_data, y_data, test_size=0.1, shuffle=True, random_state= 3)\n",
    "\n",
    "print(x_tr.shape, y_tr.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99,)\n",
      "(99,)\n",
      "[ 4.0594459e-01  6.9464952e-01 -3.1563886e-07  0.0000000e+00\n",
      "  3.6909133e-01  6.6319352e-01  1.4532449e-02  0.0000000e+00\n",
      "  3.5970673e-01  6.2063766e-01  2.2483222e-02  0.0000000e+00\n",
      "  3.6800075e-01  5.8457470e-01  2.8762527e-02  0.0000000e+00\n",
      "  3.8361007e-01  5.6840712e-01  3.5318065e-02  0.0000000e+00\n",
      "  3.8645899e-01  5.6653982e-01  1.3553927e-02  0.0000000e+00\n",
      "  3.9007059e-01  5.2962881e-01  2.0936681e-02  0.0000000e+00\n",
      "  3.9099047e-01  5.4525268e-01  2.7703114e-02  0.0000000e+00\n",
      "  3.9342180e-01  5.6385857e-01  3.1574331e-02  0.0000000e+00\n",
      "  4.1470477e-01  5.6408828e-01  7.2811381e-03  0.0000000e+00\n",
      "  4.1736990e-01  5.2783114e-01  1.2665122e-02  0.0000000e+00\n",
      "  4.1790012e-01  5.3920728e-01  1.5910219e-02  0.0000000e+00\n",
      "  4.1910914e-01  5.5208260e-01  1.7813977e-02  0.0000000e+00\n",
      "  4.3749011e-01  5.7179898e-01  1.9373042e-03  0.0000000e+00\n",
      "  4.4758251e-01  5.1945841e-01  7.3107565e-04  0.0000000e+00\n",
      "  4.5622241e-01  4.8996925e-01 -5.0639184e-03  0.0000000e+00\n",
      "  4.6328047e-01  4.6655634e-01 -1.0301309e-02  0.0000000e+00\n",
      "  4.5453674e-01  5.8519912e-01 -5.6076195e-04  0.0000000e+00\n",
      "  4.7424135e-01  5.4647446e-01 -4.8347255e-03  0.0000000e+00\n",
      "  4.8895818e-01  5.2483726e-01 -8.5862996e-03  0.0000000e+00\n",
      "  4.9922711e-01  5.0389141e-01 -1.1949016e-02  0.0000000e+00\n",
      "  3.6553513e+01  2.5007168e+01  3.0932764e+01  1.5030582e+01\n",
      "  1.4426933e+02  1.2347966e+01  5.2516174e+00  1.5473749e+02\n",
      "  7.9773359e+00  4.1079593e+00  1.0816348e+01  1.4753625e+00\n",
      "  6.1453133e+00  7.6354485e+00  8.0227146e+00]\n"
     ]
    }
   ],
   "source": [
    "print(x_tr.shape[1:3])\n",
    "print(x_tr.shape[1:2])\n",
    "print(x_tr[1:2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 64)                6400      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5)                 85        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,093\n",
      "Trainable params: 9,093\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, activation = 'relu', input_shape = x_tr.shape[1:2]),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation = 'relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(16, activation ='relu'),\n",
    "    Dense(len(actions), activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['acc']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 1.9301 - acc: 0.5732\n",
      "Epoch 1: val_acc improved from -inf to 0.98909, saving model to models/exer2\\classifier_acc_raw.h5\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.21214, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 1.6916 - acc: 0.6172 - val_loss: 0.2121 - val_acc: 0.9891 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.4054 - acc: 0.8741\n",
      "Epoch 2: val_acc improved from 0.98909 to 0.99273, saving model to models/exer2\\classifier_acc_raw.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 0.21214 to 0.04486, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.3938 - acc: 0.8766 - val_loss: 0.0449 - val_acc: 0.9927 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.2522 - acc: 0.9295\n",
      "Epoch 3: val_acc improved from 0.99273 to 0.99636, saving model to models/exer2\\classifier_acc_raw.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 0.04486 to 0.01350, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.2419 - acc: 0.9288 - val_loss: 0.0135 - val_acc: 0.9964 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.1394 - acc: 0.9609\n",
      "Epoch 4: val_acc did not improve from 0.99636\n",
      "\n",
      "Epoch 4: val_loss improved from 0.01350 to 0.00953, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.1411 - acc: 0.9616 - val_loss: 0.0095 - val_acc: 0.9964 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.1263 - acc: 0.9652\n",
      "Epoch 5: val_acc did not improve from 0.99636\n",
      "\n",
      "Epoch 5: val_loss improved from 0.00953 to 0.00825, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.1201 - acc: 0.9668 - val_loss: 0.0083 - val_acc: 0.9964 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 0.0883 - acc: 0.9727\n",
      "Epoch 6: val_acc improved from 0.99636 to 1.00000, saving model to models/exer2\\classifier_acc_raw.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 0.00825 to 0.00329, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0890 - acc: 0.9729 - val_loss: 0.0033 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0677 - acc: 0.9806\n",
      "Epoch 7: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 7: val_loss improved from 0.00329 to 0.00154, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0667 - acc: 0.9806 - val_loss: 0.0015 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 0.0636 - acc: 0.9805\n",
      "Epoch 8: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 8: val_loss improved from 0.00154 to 0.00033, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0611 - acc: 0.9806 - val_loss: 3.3035e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "61/78 [======================>.......] - ETA: 0s - loss: 0.0503 - acc: 0.9877\n",
      "Epoch 9: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.00033\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0502 - acc: 0.9862 - val_loss: 3.8948e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0288 - acc: 0.9904\n",
      "Epoch 10: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.00033\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0284 - acc: 0.9907 - val_loss: 5.4822e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0346 - acc: 0.9876\n",
      "Epoch 11: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 11: val_loss improved from 0.00033 to 0.00027, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0318 - acc: 0.9891 - val_loss: 2.7003e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0335 - acc: 0.9879\n",
      "Epoch 12: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 12: val_loss improved from 0.00027 to 0.00022, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0321 - acc: 0.9883 - val_loss: 2.1618e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0356 - acc: 0.9881\n",
      "Epoch 13: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00022\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0344 - acc: 0.9887 - val_loss: 3.5817e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0200 - acc: 0.9924  \n",
      "Epoch 14: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 14: val_loss improved from 0.00022 to 0.00018, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0230 - acc: 0.9927 - val_loss: 1.8453e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0251 - acc: 0.9925\n",
      "Epoch 15: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 15: val_loss improved from 0.00018 to 0.00010, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0242 - acc: 0.9927 - val_loss: 1.0065e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0169 - acc: 0.9946\n",
      "Epoch 16: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 16: val_loss improved from 0.00010 to 0.00002, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0181 - acc: 0.9939 - val_loss: 1.8835e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0164 - acc: 0.9964\n",
      "Epoch 17: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00002\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0162 - acc: 0.9960 - val_loss: 3.4654e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0240 - acc: 0.9910\n",
      "Epoch 18: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00002\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0221 - acc: 0.9923 - val_loss: 8.8155e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0137 - acc: 0.9953\n",
      "Epoch 19: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00002\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0141 - acc: 0.9951 - val_loss: 6.7365e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0134 - acc: 0.9943  \n",
      "Epoch 20: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00002\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0123 - acc: 0.9947 - val_loss: 4.0635e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0092 - acc: 0.9974\n",
      "Epoch 21: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 21: val_loss improved from 0.00002 to 0.00002, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0095 - acc: 0.9972 - val_loss: 1.8458e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "58/78 [=====================>........] - ETA: 0s - loss: 0.0157 - acc: 0.9935  \n",
      "Epoch 22: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 22: val_loss improved from 0.00002 to 0.00000, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0158 - acc: 0.9939 - val_loss: 4.0119e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0113 - acc: 0.9962\n",
      "Epoch 23: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 23: val_loss improved from 0.00000 to 0.00000, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0107 - acc: 0.9964 - val_loss: 1.5275e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0071 - acc: 0.9982  \n",
      "Epoch 24: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0066 - acc: 0.9984 - val_loss: 1.5549e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0140 - acc: 0.9965\n",
      "Epoch 25: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0137 - acc: 0.9964 - val_loss: 5.3133e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0098 - acc: 0.9965  \n",
      "Epoch 26: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0100 - acc: 0.9964 - val_loss: 8.4054e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "60/78 [======================>.......] - ETA: 0s - loss: 0.0096 - acc: 0.9958  \n",
      "Epoch 27: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 27: val_loss improved from 0.00000 to 0.00000, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0089 - acc: 0.9968 - val_loss: 5.1235e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0083 - acc: 0.9972\n",
      "Epoch 28: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0097 - acc: 0.9968 - val_loss: 7.3602e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0104 - acc: 0.9964\n",
      "Epoch 29: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0108 - acc: 0.9964 - val_loss: 8.2792e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0091 - acc: 0.9972\n",
      "Epoch 30: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 30: val_loss improved from 0.00000 to 0.00000, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0084 - acc: 0.9976 - val_loss: 2.8046e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0115 - acc: 0.9962\n",
      "Epoch 31: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0104 - acc: 0.9968 - val_loss: 3.1123e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 0.0089 - acc: 0.9966  \n",
      "Epoch 32: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0099 - acc: 0.9968 - val_loss: 4.5125e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "63/78 [=======================>......] - ETA: 0s - loss: 0.0085 - acc: 0.9970  \n",
      "Epoch 33: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 33: val_loss improved from 0.00000 to 0.00000, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0077 - acc: 0.9976 - val_loss: 2.3885e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0073 - acc: 0.9964  \n",
      "Epoch 34: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0072 - acc: 0.9964 - val_loss: 9.0418e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0109 - acc: 0.9959  \n",
      "Epoch 35: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0121 - acc: 0.9955 - val_loss: 2.6052e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0078 - acc: 0.9960  \n",
      "Epoch 36: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0079 - acc: 0.9964 - val_loss: 3.7365e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0089 - acc: 0.9966  \n",
      "Epoch 37: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0094 - acc: 0.9960 - val_loss: 2.5792e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0084 - acc: 0.9968  \n",
      "Epoch 38: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 38: val_loss improved from 0.00000 to 0.00000, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0080 - acc: 0.9968 - val_loss: 1.0490e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0069 - acc: 0.9972  \n",
      "Epoch 39: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0066 - acc: 0.9972 - val_loss: 8.1968e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0085 - acc: 0.9961  \n",
      "Epoch 40: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0079 - acc: 0.9964 - val_loss: 2.1145e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0070 - acc: 0.9968  \n",
      "Epoch 41: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0063 - acc: 0.9972 - val_loss: 3.2900e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0039 - acc: 0.9982  \n",
      "Epoch 42: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0036 - acc: 0.9984 - val_loss: 1.3048e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "61/78 [======================>.......] - ETA: 0s - loss: 0.0071 - acc: 0.9969 \n",
      "Epoch 43: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0073 - acc: 0.9968 - val_loss: 2.0764e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 44: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 1.8293e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0062 - acc: 0.9973  \n",
      "Epoch 45: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 45: val_loss improved from 0.00000 to 0.00000, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0062 - acc: 0.9972 - val_loss: 2.1674e-08 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0087 - acc: 0.9965  \n",
      "Epoch 46: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0080 - acc: 0.9968 - val_loss: 2.2975e-08 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "62/78 [======================>.......] - ETA: 0s - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 47: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 47: val_loss improved from 0.00000 to 0.00000, saving model to models/exer2\\classifier_loss_raw.h5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 1.3005e-08 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0061 - acc: 0.9982  \n",
      "Epoch 48: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0062 - acc: 0.9980 - val_loss: 2.4275e-08 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0059 - acc: 0.9986  \n",
      "Epoch 49: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0054 - acc: 0.9988 - val_loss: 3.5761e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9991  \n",
      "Epoch 50: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0035 - acc: 0.9992 - val_loss: 2.4709e-08 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0038 - acc: 0.9982\n",
      "Epoch 51: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0055 - acc: 0.9980 - val_loss: 4.7684e-08 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0115 - acc: 0.9982  \n",
      "Epoch 52: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0107 - acc: 0.9984 - val_loss: 3.4245e-08 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0094 - acc: 0.9958  \n",
      "Epoch 53: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0081 - acc: 0.9964 - val_loss: 7.7160e-08 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0081 - acc: 0.9982  \n",
      "Epoch 54: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0073 - acc: 0.9984 - val_loss: 5.6090e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0104 - acc: 0.9976  \n",
      "Epoch 55: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0090 - acc: 0.9980 - val_loss: 1.3741e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0020 - acc: 0.9995  \n",
      "Epoch 56: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0050 - acc: 0.9988 - val_loss: 3.8752e-07 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0015 - acc: 1.0000   \n",
      "Epoch 57: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 3.8059e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 58/1000\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 0.0073 - acc: 0.9976\n",
      "Epoch 58: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0077 - acc: 0.9976 - val_loss: 4.0226e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 59/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0042 - acc: 0.9982  \n",
      "Epoch 59: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0064 - acc: 0.9980 - val_loss: 4.3824e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 60/1000\n",
      "63/78 [=======================>......] - ETA: 0s - loss: 0.0052 - acc: 0.9990  \n",
      "Epoch 60: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0057 - acc: 0.9988 - val_loss: 4.2610e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 61/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0038 - acc: 0.9982\n",
      "Epoch 61: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0036 - acc: 0.9984 - val_loss: 4.4821e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 62/1000\n",
      "61/78 [======================>.......] - ETA: 0s - loss: 0.0015 - acc: 0.9995    \n",
      "Epoch 62: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0015 - acc: 0.9996 - val_loss: 4.4994e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 63/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0017 - acc: 1.0000  \n",
      "Epoch 63: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 4.3910e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 64/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0013 - acc: 1.0000  \n",
      "Epoch 64: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 4.0053e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 65/1000\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0042 - acc: 0.9986 \n",
      "Epoch 65: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0040 - acc: 0.9984 - val_loss: 3.9749e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 66/1000\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 4.6666e-04 - acc: 1.0000\n",
      "Epoch 66: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0015 - acc: 0.9996 - val_loss: 3.9706e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 67/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0040 - acc: 0.9991  \n",
      "Epoch 67: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0045 - acc: 0.9988 - val_loss: 3.5848e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 68/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0054 - acc: 0.9976  \n",
      "Epoch 68: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0050 - acc: 0.9980 - val_loss: 3.4634e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 69/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0035 - acc: 0.9991   \n",
      "Epoch 69: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0034 - acc: 0.9992 - val_loss: 3.0170e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 70/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0021 - acc: 0.9991  \n",
      "Epoch 70: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0025 - acc: 0.9992 - val_loss: 2.8783e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 71/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0037 - acc: 0.9991  \n",
      "Epoch 71: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0032 - acc: 0.9992 - val_loss: 2.9693e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 72/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9996   \n",
      "Epoch 72: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 0.9996 - val_loss: 2.8393e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 73/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0019 - acc: 0.9995  \n",
      "Epoch 73: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0021 - acc: 0.9992 - val_loss: 2.9996e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 74/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0048 - acc: 0.9987   \n",
      "Epoch 74: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0049 - acc: 0.9984 - val_loss: 3.1253e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 75/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0026 - acc: 0.9986  \n",
      "Epoch 75: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0025 - acc: 0.9988 - val_loss: 3.0907e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 76/1000\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 0.0027 - acc: 0.9990  \n",
      "Epoch 76: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 3.1687e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 77/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 4.0782e-04 - acc: 1.0000\n",
      "Epoch 77: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 4.6084e-04 - acc: 1.0000 - val_loss: 3.0473e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 78/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0045 - acc: 0.9981   \n",
      "Epoch 78: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0040 - acc: 0.9984 - val_loss: 2.8263e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 79/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0022 - acc: 0.9991\n",
      "Epoch 79: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0020 - acc: 0.9992 - val_loss: 2.8263e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 80/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0022 - acc: 0.9991  \n",
      "Epoch 80: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0022 - acc: 0.9992 - val_loss: 2.6572e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 81/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0047 - acc: 0.9982   \n",
      "Epoch 81: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0043 - acc: 0.9984 - val_loss: 2.1934e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 82/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0037 - acc: 0.9982  \n",
      "Epoch 82: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0034 - acc: 0.9984 - val_loss: 2.0937e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 83/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0030 - acc: 0.9986\n",
      "Epoch 83: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0034 - acc: 0.9988 - val_loss: 2.0894e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 84/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0036 - acc: 0.9987  \n",
      "Epoch 84: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0048 - acc: 0.9984 - val_loss: 2.2628e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 85/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0049 - acc: 0.9991  \n",
      "Epoch 85: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0046 - acc: 0.9992 - val_loss: 1.8596e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 86/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0026 - acc: 0.9991   \n",
      "Epoch 86: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0031 - acc: 0.9988 - val_loss: 1.9116e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 87/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0043 - acc: 0.9982\n",
      "Epoch 87: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0039 - acc: 0.9984 - val_loss: 1.9550e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 88/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0027 - acc: 0.9991    \n",
      "Epoch 88: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0025 - acc: 0.9992 - val_loss: 1.9377e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 89/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0044 - acc: 0.9986  \n",
      "Epoch 89: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0041 - acc: 0.9988 - val_loss: 1.7989e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 90/1000\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0039 - acc: 0.9986  \n",
      "Epoch 90: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0041 - acc: 0.9988 - val_loss: 1.7989e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 91/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0015 - acc: 0.9995   \n",
      "Epoch 91: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0014 - acc: 0.9996 - val_loss: 1.6429e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 92/1000\n",
      "63/78 [=======================>......] - ETA: 0s - loss: 0.0046 - acc: 0.9980\n",
      "Epoch 92: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0038 - acc: 0.9984 - val_loss: 1.7556e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 93/1000\n",
      "60/78 [======================>.......] - ETA: 0s - loss: 0.0024 - acc: 0.9990\n",
      "Epoch 93: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0020 - acc: 0.9992 - val_loss: 1.6992e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 94/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0028 - acc: 0.9995   \n",
      "Epoch 94: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0025 - acc: 0.9996 - val_loss: 1.3221e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 95/1000\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0019 - acc: 0.9990   \n",
      "Epoch 95: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0021 - acc: 0.9988 - val_loss: 1.2528e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 96/1000\n",
      "61/78 [======================>.......] - ETA: 0s - loss: 0.0013 - acc: 1.0000   \n",
      "Epoch 96: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 1.1964e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 97/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0034 - acc: 0.9986   \n",
      "Epoch 97: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 1.1791e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 98/1000\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 9.6105e-04 - acc: 0.9995\n",
      "Epoch 98: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 8.1055e-04 - acc: 0.9996 - val_loss: 1.1357e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 99/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0056 - acc: 0.9981  \n",
      "Epoch 99: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0054 - acc: 0.9984 - val_loss: 1.0707e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 100/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0012 - acc: 0.9996  \n",
      "Epoch 100: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 1.0534e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 101/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0038 - acc: 0.9986  \n",
      "Epoch 101: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0035 - acc: 0.9988 - val_loss: 1.1574e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 102/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0018 - acc: 0.9995   \n",
      "Epoch 102: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 1.2874e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 103/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0021 - acc: 0.9991 \n",
      "Epoch 103: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0024 - acc: 0.9992 - val_loss: 1.2658e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 104/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0019 - acc: 0.9996  \n",
      "Epoch 104: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0020 - acc: 0.9996 - val_loss: 1.2354e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 105/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0022 - acc: 1.0000  \n",
      "Epoch 105: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 1.1704e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 106/1000\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9992  \n",
      "Epoch 106: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 2.5000001187436284e-06.\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0028 - acc: 0.9992 - val_loss: 1.2268e-07 - val_acc: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 107/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0023 - acc: 0.9986  \n",
      "Epoch 107: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0020 - acc: 0.9988 - val_loss: 1.2268e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 108/1000\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0031 - acc: 0.9991   \n",
      "Epoch 108: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0029 - acc: 0.9992 - val_loss: 1.2354e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 109/1000\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0021 - acc: 0.9996  \n",
      "Epoch 109: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0020 - acc: 0.9996 - val_loss: 1.2311e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 110/1000\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0016 - acc: 0.9992  \n",
      "Epoch 110: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0015 - acc: 0.9992 - val_loss: 1.2224e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 111/1000\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9988  \n",
      "Epoch 111: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0036 - acc: 0.9988 - val_loss: 1.2051e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 112/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0017 - acc: 0.9991    \n",
      "Epoch 112: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0018 - acc: 0.9992 - val_loss: 1.2094e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 113/1000\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9996  \n",
      "Epoch 113: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0034 - acc: 0.9996 - val_loss: 1.2181e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 114/1000\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9996   \n",
      "Epoch 114: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0018 - acc: 0.9996 - val_loss: 1.2181e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 115/1000\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9988\n",
      "Epoch 115: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0059 - acc: 0.9988 - val_loss: 1.2137e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 116/1000\n",
      "53/78 [===================>..........] - ETA: 0s - loss: 0.0017 - acc: 0.9994    \n",
      "Epoch 116: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0025 - acc: 0.9996 - val_loss: 1.2181e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 117/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0061 - acc: 0.9978  \n",
      "Epoch 117: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0057 - acc: 0.9980 - val_loss: 1.2181e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 118/1000\n",
      "53/78 [===================>..........] - ETA: 0s - loss: 2.6284e-04 - acc: 1.0000\n",
      "Epoch 118: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 8.7242e-04 - acc: 0.9996 - val_loss: 1.2137e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 119/1000\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0017 - acc: 0.9992  \n",
      "Epoch 119: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0017 - acc: 0.9992 - val_loss: 1.2094e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 120/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0039 - acc: 0.9987  \n",
      "Epoch 120: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0041 - acc: 0.9988 - val_loss: 1.2007e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 121/1000\n",
      "59/78 [=====================>........] - ETA: 0s - loss: 3.4060e-04 - acc: 1.0000\n",
      "Epoch 121: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 5.7342e-04 - acc: 1.0000 - val_loss: 1.1964e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 122/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 9.1008e-04 - acc: 1.0000\n",
      "Epoch 122: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 8.1063e-04 - acc: 1.0000 - val_loss: 1.1964e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 123/1000\n",
      "62/78 [======================>.......] - ETA: 0s - loss: 0.0020 - acc: 0.9995    \n",
      "Epoch 123: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 1.1964e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 124/1000\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 0.0016 - acc: 0.9990  \n",
      "Epoch 124: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0014 - acc: 0.9992 - val_loss: 1.1964e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 125/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0042 - acc: 0.9986  \n",
      "Epoch 125: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0038 - acc: 0.9988 - val_loss: 1.1964e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 126/1000\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0046 - acc: 0.9981  \n",
      "Epoch 126: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0040 - acc: 0.9984 - val_loss: 1.1964e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 127/1000\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0019 - acc: 0.9995    \n",
      "Epoch 127: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0033 - acc: 0.9992 - val_loss: 1.1834e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 128/1000\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0037 - acc: 0.9983\n",
      "Epoch 128: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0035 - acc: 0.9984 - val_loss: 1.1704e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 129/1000\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0021 - acc: 0.9996 \n",
      "Epoch 129: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0021 - acc: 0.9996 - val_loss: 1.1661e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 130/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0057 - acc: 0.9974  \n",
      "Epoch 130: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0053 - acc: 0.9976 - val_loss: 1.1704e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 131/1000\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0017 - acc: 0.9996  \n",
      "Epoch 131: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0017 - acc: 0.9996 - val_loss: 1.1617e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 132/1000\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0039 - acc: 0.9979\n",
      "Epoch 132: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0037 - acc: 0.9980 - val_loss: 1.1314e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 133/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0014 - acc: 0.9996    \n",
      "Epoch 133: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 1.1227e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 134/1000\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9992   \n",
      "Epoch 134: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0029 - acc: 0.9992 - val_loss: 1.1227e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 135/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0046 - acc: 0.9982  \n",
      "Epoch 135: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0045 - acc: 0.9984 - val_loss: 1.1184e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 136/1000\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9992  \n",
      "Epoch 136: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0019 - acc: 0.9992 - val_loss: 1.1054e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 137/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0042 - acc: 0.9982\n",
      "Epoch 137: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0042 - acc: 0.9980 - val_loss: 1.1054e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 138/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0039 - acc: 0.9977  \n",
      "Epoch 138: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0049 - acc: 0.9976 - val_loss: 1.1097e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 139/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0029 - acc: 0.9986    \n",
      "Epoch 139: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0032 - acc: 0.9988 - val_loss: 1.1401e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 140/1000\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 0.0022 - acc: 0.9995   \n",
      "Epoch 140: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0019 - acc: 0.9996 - val_loss: 1.1401e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 141/1000\n",
      "63/78 [=======================>......] - ETA: 0s - loss: 0.0010 - acc: 1.0000    \n",
      "Epoch 141: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 8.4418e-04 - acc: 1.0000 - val_loss: 1.1357e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 142/1000\n",
      "62/78 [======================>.......] - ETA: 0s - loss: 0.0039 - acc: 0.9980    \n",
      "Epoch 142: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0032 - acc: 0.9984 - val_loss: 1.1314e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 143/1000\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 6.6713e-04 - acc: 1.0000\n",
      "Epoch 143: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 5.6528e-04 - acc: 1.0000 - val_loss: 1.1314e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 144/1000\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0051 - acc: 0.9990  \n",
      "Epoch 144: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0043 - acc: 0.9992 - val_loss: 1.0924e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 145/1000\n",
      "60/78 [======================>.......] - ETA: 0s - loss: 0.0031 - acc: 0.9990   \n",
      "Epoch 145: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 1.0880e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 146/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0022 - acc: 0.9995    \n",
      "Epoch 146: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0029 - acc: 0.9992 - val_loss: 1.0837e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 147/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0015 - acc: 0.9996   \n",
      "Epoch 147: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0014 - acc: 0.9996 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 148/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0015 - acc: 1.0000   \n",
      "Epoch 148: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0040 - acc: 0.9996 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 149/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0022 - acc: 0.9996\n",
      "Epoch 149: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0020 - acc: 0.9996 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 150/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0032 - acc: 0.9991  \n",
      "Epoch 150: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0028 - acc: 0.9992 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 151/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0026 - acc: 0.9991  \n",
      "Epoch 151: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 1.0880e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 152/1000\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0041 - acc: 0.9983  \n",
      "Epoch 152: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0043 - acc: 0.9984 - val_loss: 1.0880e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 153/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0020 - acc: 0.9991\n",
      "Epoch 153: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0018 - acc: 0.9992 - val_loss: 1.0837e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 154/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0018 - acc: 0.9986  \n",
      "Epoch 154: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0029 - acc: 0.9984 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 155/1000\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 155: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0036 - acc: 0.9992 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 156/1000\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0021 - acc: 0.9988    \n",
      "Epoch 156: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 1.2500000821091816e-07.\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0021 - acc: 0.9988 - val_loss: 1.0837e-07 - val_acc: 1.0000 - lr: 2.5000e-06\n",
      "Epoch 157/1000\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9992  \n",
      "Epoch 157: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0032 - acc: 0.9992 - val_loss: 1.0837e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 158/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0031 - acc: 0.9981  \n",
      "Epoch 158: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0028 - acc: 0.9984 - val_loss: 1.0837e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 159/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0038 - acc: 0.9995   \n",
      "Epoch 159: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0033 - acc: 0.9996 - val_loss: 1.0837e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 160/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0044 - acc: 0.9986  \n",
      "Epoch 160: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0048 - acc: 0.9984 - val_loss: 1.0837e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 161/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0027 - acc: 1.0000  \n",
      "Epoch 161: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 1.0837e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 162/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0035 - acc: 0.9987  \n",
      "Epoch 162: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0032 - acc: 0.9988 - val_loss: 1.0837e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 163/1000\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0033 - acc: 0.9987  \n",
      "Epoch 163: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0031 - acc: 0.9988 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 164/1000\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9984 \n",
      "Epoch 164: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0029 - acc: 0.9984 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 165/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0017 - acc: 0.9995    \n",
      "Epoch 165: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 166/1000\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9991 \n",
      "Epoch 166: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0024 - acc: 0.9992 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 167/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0042 - acc: 0.9991  \n",
      "Epoch 167: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0042 - acc: 0.9988 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 168/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0025 - acc: 0.9996  \n",
      "Epoch 168: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0023 - acc: 0.9996 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 169/1000\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0028 - acc: 0.9991  \n",
      "Epoch 169: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 0.9992 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 170/1000\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0042 - acc: 0.9983\n",
      "Epoch 170: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0041 - acc: 0.9984 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 171/1000\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 8.7357e-04 - acc: 1.0000\n",
      "Epoch 171: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 172/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0036 - acc: 0.9986  \n",
      "Epoch 172: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0032 - acc: 0.9988 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 173/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0042 - acc: 0.9986   \n",
      "Epoch 173: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0037 - acc: 0.9988 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 174/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0026 - acc: 0.9991\n",
      "Epoch 174: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0025 - acc: 0.9992 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 175/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0036 - acc: 0.9991  \n",
      "Epoch 175: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0038 - acc: 0.9992 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 176/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0025 - acc: 0.9991\n",
      "Epoch 176: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 177/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0020 - acc: 0.9995 \n",
      "Epoch 177: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0044 - acc: 0.9988 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 178/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0013 - acc: 0.9995  \n",
      "Epoch 178: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 0.9992 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 179/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0045 - acc: 0.9991  \n",
      "Epoch 179: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0041 - acc: 0.9992 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 180/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0021 - acc: 0.9991    \n",
      "Epoch 180: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0020 - acc: 0.9992 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 181/1000\n",
      "58/78 [=====================>........] - ETA: 0s - loss: 0.0014 - acc: 0.9989  \n",
      "Epoch 181: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0011 - acc: 0.9992 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 182/1000\n",
      "76/78 [============================>.] - ETA: 0s - loss: 4.4843e-04 - acc: 1.0000\n",
      "Epoch 182: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 4.4137e-04 - acc: 1.0000 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 183/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0032 - acc: 0.9991 \n",
      "Epoch 183: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0028 - acc: 0.9992 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 184/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 4.7886e-04 - acc: 1.0000\n",
      "Epoch 184: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 4.3913e-04 - acc: 1.0000 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 185/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0011 - acc: 0.9995    \n",
      "Epoch 185: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 186/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0025 - acc: 0.9991\n",
      "Epoch 186: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 187/1000\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0041 - acc: 0.9983  \n",
      "Epoch 187: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0039 - acc: 0.9984 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 188/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0027 - acc: 0.9991  \n",
      "Epoch 188: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0028 - acc: 0.9992 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 189/1000\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0017 - acc: 0.9996  \n",
      "Epoch 189: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 190/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 6.5039e-04 - acc: 1.0000\n",
      "Epoch 190: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 7.9299e-04 - acc: 1.0000 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 191/1000\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9992   \n",
      "Epoch 191: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0020 - acc: 0.9992 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 192/1000\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0017 - acc: 0.9992 \n",
      "Epoch 192: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0016 - acc: 0.9992 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 193/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0045 - acc: 0.9991  \n",
      "Epoch 193: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0049 - acc: 0.9988 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 194/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0052 - acc: 0.9977\n",
      "Epoch 194: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0053 - acc: 0.9976 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 195/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0043 - acc: 0.9982  \n",
      "Epoch 195: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0040 - acc: 0.9984 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 196/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0066 - acc: 0.9978\n",
      "Epoch 196: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0060 - acc: 0.9980 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 197/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0016 - acc: 1.0000   \n",
      "Epoch 197: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0020 - acc: 0.9996 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 198/1000\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 9.6559e-04 - acc: 0.9996\n",
      "Epoch 198: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 199/1000\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0011 - acc: 0.9996   \n",
      "Epoch 199: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0010 - acc: 0.9996 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 200/1000\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0051 - acc: 0.9980   \n",
      "Epoch 200: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0051 - acc: 0.9980 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 201/1000\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 0.0027 - acc: 0.9990  \n",
      "Epoch 201: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 1.0794e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 202/1000\n",
      "62/78 [======================>.......] - ETA: 0s - loss: 0.0047 - acc: 0.9980 \n",
      "Epoch 202: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0039 - acc: 0.9984 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 203/1000\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 5.7815e-04 - acc: 1.0000\n",
      "Epoch 203: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 5.4700e-04 - acc: 1.0000 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 204/1000\n",
      "57/78 [====================>.........] - ETA: 0s - loss: 0.0043 - acc: 0.9984  \n",
      "Epoch 204: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0034 - acc: 0.9988 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 205/1000\n",
      "63/78 [=======================>......] - ETA: 0s - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 205: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 206/1000\n",
      "63/78 [=======================>......] - ETA: 0s - loss: 5.7741e-04 - acc: 1.0000\n",
      "Epoch 206: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 206: ReduceLROnPlateau reducing learning rate to 6.250000694763003e-09.\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 1.2500e-07\n",
      "Epoch 207/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0041 - acc: 0.9986   \n",
      "Epoch 207: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0038 - acc: 0.9988 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 208/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0042 - acc: 0.9986\n",
      "Epoch 208: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0038 - acc: 0.9988 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 209/1000\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0040 - acc: 0.9981  \n",
      "Epoch 209: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0040 - acc: 0.9980 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 210/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0020 - acc: 1.0000   \n",
      "Epoch 210: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 211/1000\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 5.5134e-04 - acc: 1.0000\n",
      "Epoch 211: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 212/1000\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0043 - acc: 0.9983\n",
      "Epoch 212: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0041 - acc: 0.9984 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 213/1000\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0017 - acc: 0.9992    \n",
      "Epoch 213: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0022 - acc: 0.9992 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 214/1000\n",
      "76/78 [============================>.] - ETA: 0s - loss: 3.3726e-04 - acc: 1.0000\n",
      "Epoch 214: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 3.3213e-04 - acc: 1.0000 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 215/1000\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0046 - acc: 0.9983  \n",
      "Epoch 215: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0048 - acc: 0.9984 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 216/1000\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0038 - acc: 0.9987  \n",
      "Epoch 216: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0042 - acc: 0.9984 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 217/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0011 - acc: 1.0000    \n",
      "Epoch 217: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 218/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0021 - acc: 0.9991\n",
      "Epoch 218: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0020 - acc: 0.9992 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 219/1000\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0029 - acc: 0.9986\n",
      "Epoch 219: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 0.9988 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 220/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0052 - acc: 0.9986  \n",
      "Epoch 220: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0059 - acc: 0.9980 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 221/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0045 - acc: 0.9982  \n",
      "Epoch 221: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0059 - acc: 0.9980 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 222/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0015 - acc: 0.9991   \n",
      "Epoch 222: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0013 - acc: 0.9992 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 223/1000\n",
      "62/78 [======================>.......] - ETA: 0s - loss: 0.0018 - acc: 0.9995    \n",
      "Epoch 223: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 224/1000\n",
      "60/78 [======================>.......] - ETA: 0s - loss: 0.0019 - acc: 0.9990  \n",
      "Epoch 224: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0025 - acc: 0.9988 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 225/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0063 - acc: 0.9977  \n",
      "Epoch 225: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0058 - acc: 0.9980 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 226/1000\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0039 - acc: 0.9983 \n",
      "Epoch 226: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0036 - acc: 0.9984 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 227/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0013 - acc: 0.9995    \n",
      "Epoch 227: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 228/1000\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9980\n",
      "Epoch 228: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0048 - acc: 0.9980 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 229/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0024 - acc: 0.9996  \n",
      "Epoch 229: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0023 - acc: 0.9996 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 230/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0031 - acc: 0.9991  \n",
      "Epoch 230: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0029 - acc: 0.9992 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 231/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0026 - acc: 0.9991  \n",
      "Epoch 231: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0024 - acc: 0.9992 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 232/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0043 - acc: 0.9978  \n",
      "Epoch 232: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0042 - acc: 0.9980 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 233/1000\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9991  \n",
      "Epoch 233: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0021 - acc: 0.9992 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 234/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0022 - acc: 0.9995 \n",
      "Epoch 234: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0025 - acc: 0.9992 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 235/1000\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9992 \n",
      "Epoch 235: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0028 - acc: 0.9992 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 236/1000\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0065 - acc: 0.9978  \n",
      "Epoch 236: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0061 - acc: 0.9980 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 237/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0024 - acc: 0.9996   \n",
      "Epoch 237: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0031 - acc: 0.9992 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 238/1000\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 1.0000    \n",
      "Epoch 238: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 239/1000\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0035 - acc: 0.9991  \n",
      "Epoch 239: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0031 - acc: 0.9992 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 240/1000\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0040 - acc: 0.9988  \n",
      "Epoch 240: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0040 - acc: 0.9988 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 241/1000\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9992    \n",
      "Epoch 241: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0015 - acc: 0.9992 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 242/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0016 - acc: 0.9996  \n",
      "Epoch 242: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0015 - acc: 0.9996 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 243/1000\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0036 - acc: 0.9991    \n",
      "Epoch 243: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0039 - acc: 0.9988 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 244/1000\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0039 - acc: 0.9991  \n",
      "Epoch 244: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0038 - acc: 0.9992 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 245/1000\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0034 - acc: 0.9995\n",
      "Epoch 245: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0031 - acc: 0.9996 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 246/1000\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0027 - acc: 0.9986  \n",
      "Epoch 246: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0036 - acc: 0.9984 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n",
      "Epoch 247/1000\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0018 - acc: 0.9996   \n",
      "Epoch 247: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.00000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 1.0750e-07 - val_acc: 1.0000 - lr: 6.2500e-09\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_tr, y_tr,\n",
    "    validation_data = (x_val, y_val),\n",
    "    epochs = 1000,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(f'models/{exercise}/classifier_acc_raw.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "        ModelCheckpoint(f'models/{exercise}/classifier_loss_raw.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto'),\n",
    "        ReduceLROnPlateau(monitor='val_acc', factor=0.05, patience=50, verbose=1, mode='auto'),\n",
    "        EarlyStopping(monitor='val_loss', min_delta=0, patience=200, mode='auto')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, loss_ax = plt.subplots(figsize=(16,10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'],'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['acc'],'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'],'g',label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff4f85d6e04298634172ac5d8264e7e9b556b95639fe52ebb9425c4d4cba0c9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
